# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "client<llm> LlamaNebius {\r\n  provider openai\r\n  options {\r\n    model \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \r\n    api_key env.NEBIUS_API_KEY\r\n    base_url \"https://api.studio.nebius.com/v1/\"\r\n    temperature 0.0\r\n  }\r\n}  \r\n\r\nclient<llm> LlamaNebiusCI {\r\n  provider openai\r\n  options {\r\n    model \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \r\n    api_key env.NEBIUS_API_KEY\r\n    base_url \"https://api.studio.nebius.com/v1/\"\r\n    temperature 0.7\r\n  }\r\n}  ",
    "form_completion.baml": "class TextInput {\r\n  text string\r\n}\r\n\r\nclass PersonalInfo {\r\n  first_name string\r\n  last_name string\r\n  gender string?\r\n}\r\n\r\nclass ContactInfo {\r\n  email string\r\n  phone string?\r\n  preferred_contact_method string\r\n  call_reasons string[]?\r\n}\r\n\r\nclass FinalFormInformations {\r\n  personal_info PersonalInfo\r\n  contact_info ContactInfo\r\n}\r\n\r\nfunction ExtractFormData(text: string) -> FinalFormInformations {\r\n  client LlamaNebius\r\n  prompt #\"\r\n    Analyze the following conversation between an agent and a customer to extract customer information.\r\n    \r\n    Conversation: {{ text }}\r\n    \r\n    Extract the following information from the conversation and return it in the exact JSON format specified:\r\n    - First name and last name of the customer, assign to first_name the token most likely to be the first name.\r\n    - Gender (if mentioned, otherwise null)\r\n    - Email address\r\n    - Phone number (if mentioned, otherwise null)\r\n    - Preferred contact method (email or phone)\r\n    - Reasons for the call (if mentioned, otherwise null)\r\n    \r\n    Return the data in this exact structure:\r\n    {\r\n      \"personal_info\": {\r\n        \"first_name\": \"string\",\r\n        \"last_name\": \"string\", \r\n        \"gender\": \"Male, Female or Other\"\r\n      },\r\n      \"contact_info\": {\r\n        \"email\": \"string\",\r\n        \"phone\": \"string or null\",\r\n        \"preferred_contact_method\": \"Email or Phone\",\r\n        \"call_reasons\": \"string or null\"\r\n      }\r\n    }\r\n    \r\n    {{ \"{{ ctx.output_format }}\" }}\r\n  \"#\r\n}\r\n\r\n",
    "generalized_form_completion.baml": "class TextInputGen {\r\n  text string\r\n  form_schema string\r\n}\r\n\r\nfunction ExtractFormDataGen(input: TextInputGen) -> string {\r\n  client LlamaNebius\r\n  prompt #\"\r\n    Analyze the following conversation between an agent and a customer to extract customer information.\r\n    \r\n    Conversation: {{input.text}}\r\n    \r\n    Based on the schema below, extract the relevant fields and fill the values accordingly. If you can't find the information, set it up to NULL value\r\n\r\n    Schema: {{input.form_schema}}\r\n\r\n    Don't add any explanation. \r\n\r\n    {{ \"{{ ctx.output_format }}\" }}\r\n  \"#\r\n}\r\n\r\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.203.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "streamed_form_completion.baml": "class TextInputStreamed {\r\n  text string\r\n  form_schema string\r\n}\r\n\r\nfunction ExtractFormDataStreamed(input: TextInputStreamed) -> string {\r\n  client LlamaNebius\r\n  prompt #\"\r\n    Analyze the following conversation between an agent and a customer to extract customer information.\r\n    \r\n    Conversation: {{input.text}}\r\n    \r\n    Based on the schema below, extract the relevant fields and fill the values accordingly. If you can't find the information, set it up to NULL value\r\n\r\n    Schema: {{input.form_schema}}\r\n\r\n    Don't add any explanation. \r\n\r\n    {{ \"{{ ctx.output_format }}\" }}\r\n  \"#\r\n}\r\n\r\n",
    "text_classification.baml": "class Theme {\r\n  title string\r\n  description string\r\n}\r\n\r\nclass ClassificationResult {\r\n  model_reasoning string\r\n  chosen_theme Theme\r\n}\r\n\r\nfunction TextClassification(text: string, themes: Theme[]) -> ClassificationResult {\r\n  client LlamaNebius\r\n  prompt #\"\r\n    {# \r\n      Prompts are auto-dedented and trimmed.\r\n      We use JINJA for our prompt syntax\r\n      (but we added some static analysis to make sure it's valid!)\r\n    #}\r\n\r\n    {{ ctx.output_format(prefix=\"Classify with the following json:\") }}\r\n\r\n    You are a helpful assistant that helps classifying texts in given themes\r\n    Available themes:\r\n    {% for theme in themes %}\r\n    - {{ theme.title }}: {{ theme.description }}\r\n    {% endfor %}\r\n\r\n    Text: {{ text }}\r\n\r\n    Classify this message and provide:\r\n    1. Your reasoning for this classification\r\n    2. The chosen theme, (don't invent theme)\r\n\r\n    {{ _.chat('assistant') }}\r\n    JSON response with chosen_theme and model_reasoning:\r\n  \"#\r\n}",
    "text_classification_confident_interval.baml": "class Theme_ci {\r\n  title string\r\n  description string\r\n}\r\n\r\nclass ClassificationResult_ci {\r\n  model_reasoning string\r\n  chosen_theme Theme\r\n}\r\n\r\nfunction TextClassification_ci(text: string, themes: Theme_ci[]) -> ClassificationResult_ci {\r\n\r\n  // Here I use the same model as the one used for the text classification, but with a different temperature\r\n  client LlamaNebiusCI\r\n  prompt #\"\r\n    {# \r\n      Prompts are auto-dedented and trimmed.\r\n      We use JINJA for our prompt syntax\r\n      (but we added some static analysis to make sure it's valid!)\r\n    #}\r\n\r\n    {{ ctx.output_format(prefix=\"Classify with the following json:\") }}\r\n\r\n    You are a helpful assistant that helps classifying texts in given themes\r\n    Available themes:\r\n    {% for theme in themes %}\r\n    - {{ theme.title }}: {{ theme.description }}\r\n    {% endfor %}\r\n\r\n    Text: {{ text }}\r\n\r\n    Classify this message and provide:\r\n    1. Your reasoning for this classification\r\n    2. The chosen theme, (don't invent theme)\r\n\r\n    {{ _.chat('assistant') }}\r\n    JSON response with chosen_theme and model_reasoning:\r\n  \"#\r\n}",
}

def get_baml_files():
    return _file_map